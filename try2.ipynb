{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparametres for ridge classifier\n",
    "# importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43508, 16) (43508,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import dataset\n",
    "dataset = pd.read_csv('csv/cleaned_train_all.csv')\n",
    "\n",
    "testdataset = pd.read_csv('csv/cleaned_test_all.csv')\n",
    "\n",
    "\n",
    "\n",
    "x = dataset.drop(['credit_card_default'], axis = 1)\n",
    "y = dataset['credit_card_default'].values\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar =  preprocessing.StandardScaler()\n",
    "\n",
    "# copy of datasets\n",
    "X_train = x.copy()\n",
    "\n",
    "# numerical features\n",
    "\n",
    "#for all \n",
    "num_cols = ['net_yearly_income','no_of_days_employed','yearly_debt_payments',\n",
    "             'credit_limit',\"credit_limit_used(%)\", \"credit_score\"]\n",
    "\n",
    "#for less\n",
    "num_cols = [\"credit_limit_used(%)\", \"credit_score\"]\n",
    "\n",
    "#apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # fit on training data column\n",
    "    scale = scalar.fit(X_train[[i]])\n",
    "    scale2 =  scalar.fit(testdataset[[i]])\n",
    "    # transform the training data column\n",
    "    X_train[i] = scale.transform(X_train[[i]])\n",
    "    testdataset[i] = scale.transform(testdataset[[i]])\n",
    "\n",
    "X_train = X_train.set_index('customer_id')\n",
    "testdataset = testdataset.set_index('customer_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size = 0.25, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier(base_estimator = None, n_estimators = 200, learning_rate = 1.0)\n",
    "adaboost.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt = GradientBoostingClassifier(loss = 'deviance', learning_rate = 0.1, n_estimators = 200, min_samples_split = 2,\n",
    "                                  min_samples_leaf = 1, max_depth = 3, subsample = 1.0)\n",
    "\n",
    "gbdt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=50; total time=   1.0s\n",
      "[CV] END ....................................n_estimators=50; total time=   1.0s\n",
      "[CV] END ....................................n_estimators=50; total time=   1.1s\n",
      "[CV] END ....................................n_estimators=50; total time=   1.1s\n",
      "[CV] END ....................................n_estimators=50; total time=   1.1s\n",
      "[CV] END ...................................n_estimators=100; total time=   2.2s\n",
      "[CV] END ...................................n_estimators=100; total time=   2.3s\n",
      "[CV] END ...................................n_estimators=100; total time=   2.2s\n",
      "[CV] END ...................................n_estimators=100; total time=   2.2s\n",
      "[CV] END ...................................n_estimators=100; total time=   2.2s\n",
      "[CV] END ...................................n_estimators=200; total time=   4.4s\n",
      "[CV] END ...................................n_estimators=200; total time=   4.4s\n",
      "[CV] END ...................................n_estimators=200; total time=   4.9s\n",
      "[CV] END ...................................n_estimators=200; total time=   4.6s\n",
      "[CV] END ...................................n_estimators=200; total time=   4.5s\n",
      "[CV] END ...................................n_estimators=400; total time=   8.9s\n",
      "[CV] END ...................................n_estimators=400; total time=   8.9s\n",
      "[CV] END ...................................n_estimators=400; total time=   9.0s\n",
      "[CV] END ...................................n_estimators=400; total time=   9.7s\n",
      "[CV] END ...................................n_estimators=400; total time=   9.7s\n"
     ]
    }
   ],
   "source": [
    "adaParams = {'n_estimators':[10,50,100,200,400]}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "ada = RandomizedSearchCV(estimator = adaboost, param_distributions = adaParams, n_iter = 5, scoring = 'roc_auc',\n",
    "                          cv = None, verbose = 2).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ADAH =ada.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score - : 0.9182317250459042\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score - :', metrics.f1_score(y_test, y_pred_ADAH , average=\"macro\" ) )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ........loss=deviance, max_depth=9, n_estimators=50; total time=   3.7s\n",
      "[CV] END ........loss=deviance, max_depth=9, n_estimators=50; total time=   3.8s\n",
      "[CV] END ........loss=deviance, max_depth=9, n_estimators=50; total time=   3.5s\n",
      "[CV] END ........loss=deviance, max_depth=9, n_estimators=50; total time=   4.1s\n",
      "[CV] END ........loss=deviance, max_depth=9, n_estimators=50; total time=   3.7s\n",
      "[CV] END ....loss=exponential, max_depth=9, n_estimators=100; total time=   9.7s\n",
      "[CV] END ....loss=exponential, max_depth=9, n_estimators=100; total time=   9.6s\n",
      "[CV] END ....loss=exponential, max_depth=9, n_estimators=100; total time=   9.4s\n",
      "[CV] END ....loss=exponential, max_depth=9, n_estimators=100; total time=   9.9s\n",
      "[CV] END ....loss=exponential, max_depth=9, n_estimators=100; total time=   9.8s\n",
      "[CV] END ........loss=deviance, max_depth=3, n_estimators=10; total time=   0.3s\n",
      "[CV] END ........loss=deviance, max_depth=3, n_estimators=10; total time=   0.3s\n",
      "[CV] END ........loss=deviance, max_depth=3, n_estimators=10; total time=   0.3s\n",
      "[CV] END ........loss=deviance, max_depth=3, n_estimators=10; total time=   0.3s\n",
      "[CV] END ........loss=deviance, max_depth=3, n_estimators=10; total time=   0.2s\n",
      "[CV] END .....loss=exponential, max_depth=5, n_estimators=50; total time=   2.7s\n",
      "[CV] END .....loss=exponential, max_depth=5, n_estimators=50; total time=   2.6s\n",
      "[CV] END .....loss=exponential, max_depth=5, n_estimators=50; total time=   2.9s\n",
      "[CV] END .....loss=exponential, max_depth=5, n_estimators=50; total time=   2.7s\n",
      "[CV] END .....loss=exponential, max_depth=5, n_estimators=50; total time=   2.5s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=200; total time=   8.4s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=200; total time=   8.5s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=200; total time=   8.4s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=200; total time=   8.4s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=200; total time=   8.4s\n",
      "[CV] END .......loss=deviance, max_depth=7, n_estimators=400; total time=  37.2s\n",
      "[CV] END .......loss=deviance, max_depth=7, n_estimators=400; total time=  37.3s\n",
      "[CV] END .......loss=deviance, max_depth=7, n_estimators=400; total time=  40.1s\n",
      "[CV] END .......loss=deviance, max_depth=7, n_estimators=400; total time=  39.1s\n",
      "[CV] END .......loss=deviance, max_depth=7, n_estimators=400; total time=  40.4s\n",
      "[CV] END ........loss=deviance, max_depth=7, n_estimators=10; total time=   0.4s\n",
      "[CV] END ........loss=deviance, max_depth=7, n_estimators=10; total time=   0.4s\n",
      "[CV] END ........loss=deviance, max_depth=7, n_estimators=10; total time=   0.4s\n",
      "[CV] END ........loss=deviance, max_depth=7, n_estimators=10; total time=   0.3s\n",
      "[CV] END ........loss=deviance, max_depth=7, n_estimators=10; total time=   0.3s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=100; total time=   7.4s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=100; total time=   6.6s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=100; total time=   6.5s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=100; total time=   6.3s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=100; total time=   6.1s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=100; total time=   4.2s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=100; total time=   4.2s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=100; total time=   4.2s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=100; total time=   4.1s\n",
      "[CV] END .......loss=deviance, max_depth=3, n_estimators=100; total time=   4.2s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=200; total time=  13.5s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=200; total time=  13.5s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=200; total time=  13.5s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=200; total time=  13.5s\n",
      "[CV] END ....loss=exponential, max_depth=5, n_estimators=200; total time=  13.3s\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter tuning for GBDT :\n",
    "from random import randint\n",
    "gbParams = {'loss': ['deviance', 'exponential'],\n",
    "            'n_estimators': [10, 50, 100, 200, 400],\n",
    "            'max_depth': [3,5,7,9]}\n",
    "\n",
    "\n",
    "gb = RandomizedSearchCV(estimator = gbdt, param_distributions = gbParams, n_iter = 10, scoring = 'roc_auc',\n",
    "                       cv = None, verbose = 2).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score - : 0.9182317250459042\n"
     ]
    }
   ],
   "source": [
    "y_pred_GBDTH =ada.predict(X_test)\n",
    "print('F1 Score - :', metrics.f1_score(y_test, y_pred_GBDTH , average=\"macro\" ) )  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
